{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Scoring Model - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on the loan dataset.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and inspect the dataset\n",
    "2. Analyze data quality (missing values, duplicates)\n",
    "3. Explore feature distributions\n",
    "4. Analyze correlations and relationships\n",
    "5. Identify patterns in loan defaults\n",
    "6. Detect outliers\n",
    "7. Assess class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Import custom utilities\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.utils import load_config, check_missing_values, detect_outliers_iqr\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../config/model_config.yaml')\n",
    "\n",
    "# Load data\n",
    "data_path = '../data/raw/loan_data.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"✓ Data loaded successfully!\")\n",
    "    print(f\"  Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ Data file not found. Please download dataset to data/raw/loan_data.csv\")\n",
    "    print(\"  Recommended datasets:\")\n",
    "    print(\"  - Kaggle: Loan Prediction Dataset\")\n",
    "    print(\"  - Kaggle: Give Me Some Credit\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Display first few rows\n",
    "    print(\"First 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Display last few rows\n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Dataset info\n",
    "    print(\"Dataset Information:\")\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Statistical summary\n",
    "    print(\"Statistical Summary:\")\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Data types\n",
    "    print(\"Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nNumeric columns:\", df.select_dtypes(include=[np.number]).columns.tolist())\n",
    "    print(\"Categorical columns:\", df.select_dtypes(include=['object', 'category']).columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Duplicate rows: {duplicates}\")\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        print(f\"Percentage of duplicates: {duplicates/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Missing values analysis\n",
    "    missing_summary = check_missing_values(df)\n",
    "    \n",
    "    if not missing_summary.empty:\n",
    "        print(\"Missing Values Summary:\")\n",
    "        display(missing_summary)\n",
    "        \n",
    "        # Visualize missing values\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        missing_summary.plot(x='Column', y='Missing_Percentage', kind='bar', ax=ax, color='coral')\n",
    "        ax.set_title('Missing Values by Column', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Column', fontsize=12)\n",
    "        ax.set_ylabel('Missing Percentage (%)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"✓ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'loan_status' in df.columns:\n",
    "    # Target distribution\n",
    "    target_counts = df['loan_status'].value_counts()\n",
    "    print(\"Loan Status Distribution:\")\n",
    "    print(target_counts)\n",
    "    print(f\"\\nDefault rate: {target_counts.get(1, 0) / len(df) * 100:.2f}%\")\n",
    "    \n",
    "    # Visualize target distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar plot\n",
    "    target_counts.plot(kind='bar', ax=axes[0], color=['green', 'red'])\n",
    "    axes[0].set_title('Loan Status Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Loan Status (0=Paid, 1=Default)', fontsize=12)\n",
    "    axes[0].set_ylabel('Count', fontsize=12)\n",
    "    axes[0].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(target_counts.values, labels=['Paid (0)', 'Default (1)'], \n",
    "                autopct='%1.1f%%', colors=['green', 'red'], startangle=90)\n",
    "    axes[1].set_title('Loan Status Proportion', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Class imbalance assessment\n",
    "    imbalance_ratio = target_counts.max() / target_counts.min()\n",
    "    print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "    \n",
    "    if imbalance_ratio > 3:\n",
    "        print(\"⚠ Significant class imbalance detected. Consider using SMOTE or class weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Numeric Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Get numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'loan_status' in numeric_cols:\n",
    "        numeric_cols.remove('loan_status')\n",
    "    \n",
    "    # Plot distributions\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows*4))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        if idx < len(axes):\n",
    "            axes[idx].hist(df[col].dropna(), bins=50, color='skyblue', edgecolor='black')\n",
    "            axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "            axes[idx].set_xlabel(col, fontsize=10)\n",
    "            axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "            axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(numeric_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Categorical Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Get categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    if categorical_cols:\n",
    "        n_cols = 2\n",
    "        n_rows = (len(categorical_cols) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows*4))\n",
    "        axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "        \n",
    "        for idx, col in enumerate(categorical_cols):\n",
    "            if idx < len(axes):\n",
    "                value_counts = df[col].value_counts()\n",
    "                axes[idx].bar(range(len(value_counts)), value_counts.values, color='coral')\n",
    "                axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "                axes[idx].set_xlabel(col, fontsize=10)\n",
    "                axes[idx].set_ylabel('Count', fontsize=10)\n",
    "                axes[idx].set_xticks(range(len(value_counts)))\n",
    "                axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "                axes[idx].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(len(categorical_cols), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No categorical columns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Compute correlation matrix\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "    ax.set_title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "    high_corr = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "                high_corr.append((\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j],\n",
    "                    correlation_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    if high_corr:\n",
    "        for feat1, feat2, corr in high_corr:\n",
    "            print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"  None found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Target vs. Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'loan_status' in df.columns:\n",
    "    # Select key numeric features\n",
    "    key_features = ['person_income', 'loan_amnt', 'loan_int_rate', 'loan_percent_income']\n",
    "    available_features = [f for f in key_features if f in df.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, feature in enumerate(available_features[:4]):\n",
    "            if idx < len(axes):\n",
    "                df.boxplot(column=feature, by='loan_status', ax=axes[idx])\n",
    "                axes[idx].set_title(f'{feature} by Loan Status', fontsize=12, fontweight='bold')\n",
    "                axes[idx].set_xlabel('Loan Status (0=Paid, 1=Default)', fontsize=10)\n",
    "                axes[idx].set_ylabel(feature, fontsize=10)\n",
    "                axes[idx].get_figure().suptitle('')  # Remove automatic title\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Detect outliers using IQR method\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'loan_status' in numeric_cols:\n",
    "        numeric_cols.remove('loan_status')\n",
    "    \n",
    "    outlier_counts = detect_outliers_iqr(df, numeric_cols, threshold=1.5)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    outlier_summary = pd.DataFrame({\n",
    "        'Feature': list(outlier_counts.keys()),\n",
    "        'Outlier_Count': list(outlier_counts.values()),\n",
    "        'Outlier_Percentage': [count/len(df)*100 for count in outlier_counts.values()]\n",
    "    }).sort_values(by='Outlier_Percentage', ascending=False)\n",
    "    \n",
    "    print(\"Outlier Summary (IQR method):\")\n",
    "    display(outlier_summary)\n",
    "    \n",
    "    # Visualize outlier percentages\n",
    "    if not outlier_summary.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.barh(outlier_summary['Feature'], outlier_summary['Outlier_Percentage'], color='orange')\n",
    "        ax.set_xlabel('Outlier Percentage (%)', fontsize=12)\n",
    "        ax.set_ylabel('Feature', fontsize=12)\n",
    "        ax.set_title('Outlier Percentage by Feature', fontsize=14, fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"KEY INSIGHTS FROM EDA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n1. Dataset Size: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_summary = check_missing_values(df)\n",
    "    if not missing_summary.empty:\n",
    "        print(f\"\\n2. Missing Values: Found in {len(missing_summary)} columns\")\n",
    "        print(f\"   Action needed: Imputation or removal\")\n",
    "    else:\n",
    "        print(f\"\\n2. Missing Values: None ✓\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"\\n3. Duplicates: {duplicates} rows ({duplicates/len(df)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n3. Duplicates: None ✓\")\n",
    "    \n",
    "    # Class imbalance\n",
    "    if 'loan_status' in df.columns:\n",
    "        target_counts = df['loan_status'].value_counts()\n",
    "        default_rate = target_counts.get(1, 0) / len(df) * 100\n",
    "        print(f\"\\n4. Default Rate: {default_rate:.2f}%\")\n",
    "        \n",
    "        imbalance_ratio = target_counts.max() / target_counts.min()\n",
    "        print(f\"   Class imbalance: {imbalance_ratio:.2f}:1\")\n",
    "        if imbalance_ratio > 3:\n",
    "            print(\"   Recommendation: Apply SMOTE or class weights\")\n",
    "    \n",
    "    # Outliers\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'loan_status' in numeric_cols:\n",
    "        numeric_cols.remove('loan_status')\n",
    "    outlier_counts = detect_outliers_iqr(df, numeric_cols, threshold=1.5)\n",
    "    total_outliers = sum(outlier_counts.values())\n",
    "    print(f\"\\n5. Outliers: {total_outliers} detected across {len(numeric_cols)} features\")\n",
    "    print(\"   Recommendation: Cap outliers at IQR boundaries\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NEXT STEPS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"1. Run data preprocessing (src/data_preprocessing.py)\")\n",
    "    print(\"2. Engineer risk tiers (Low, Medium, High)\")\n",
    "    print(\"3. Apply feature engineering and selection\")\n",
    "    print(\"4. Train and evaluate models\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save EDA Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Create EDA report\n",
    "    report = {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'numeric_features': len(df.select_dtypes(include=[np.number]).columns),\n",
    "        'categorical_features': len(df.select_dtypes(include=['object', 'category']).columns),\n",
    "        'missing_values': df.isnull().sum().sum(),\n",
    "        'duplicate_rows': df.duplicated().sum(),\n",
    "    }\n",
    "    \n",
    "    if 'loan_status' in df.columns:\n",
    "        target_counts = df['loan_status'].value_counts()\n",
    "        report['default_rate'] = target_counts.get(1, 0) / len(df) * 100\n",
    "        report['class_imbalance_ratio'] = target_counts.max() / target_counts.min()\n",
    "    \n",
    "    # Save report\n",
    "    report_df = pd.DataFrame([report])\n",
    "    report_path = '../outputs/reports/eda_summary.csv'\n",
    "    report_df.to_csv(report_path, index=False)\n",
    "    \n",
    "    print(f\"✓ EDA report saved to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
